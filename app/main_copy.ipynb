{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":31283,"status":"ok","timestamp":1684398481979,"user":{"displayName":"佐藤太洋","userId":"13036286077109574300"},"user_tz":-540},"id":"-QrsJIci8koH","outputId":"140d6694-0193-4269-cb38-a8b87dbd40c5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/Othercomputers/LAPTOP-3BHDM0TI/windows_research/workspace/research/carbon-steel-classification\n","total 28\n","drwx------ 2 root root 4096 May  8 09:43 app\n","drwx------ 2 root root 4096 May  8 09:52 bin\n","drwx------ 2 root root 4096 May  9 20:26 data\n","drwx------ 2 root root 4096 May 10 06:28 dataset_npz\n","drwx------ 2 root root 4096 May  8 09:16 .git\n","-rw------- 1 root root   48 May  8 09:58 .gitignore\n","-rw------- 1 root root  265 May  8 09:51 Makefile\n","-rw------- 1 root root  358 May  8 09:37 README.md\n","drwx------ 2 root root 4096 May  8 09:38 tmp\n","-rw------- 1 root root 2220 May 18 08:23 TODO.txt\n"]}],"source":["# ドライブのマウント\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# 適宜自分のApplicationPATHまで変更\n","APP_PATH = '/content/drive/Othercomputers/LAPTOP-3BHDM0TI/windows_research/workspace/research/carbon-steel-classification'\n","\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import torch\n","import torchvision\n","import torchvision.transforms as transforms\n","from torch.utils.data import TensorDataset, DataLoader\n","from torch import nn\n","import torch.nn.functional as F\n","from torch import optim\n","\n","from sklearn.model_selection import train_test_split\n","\n","import os\n","import sys\n","import cv2\n","from PIL import Image\n","import pandas as pd\n","\n","\n","os.chdir(APP_PATH)\n","!pwd\n","!ls -al"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"np6tIMs5_Lz8"},"outputs":[],"source":["# ファイルの実行(ノートブック側でランタイムの再接続 or reloadをする)\n","# importで実行するときはファイルのパスを通す必要がある\n","sys.path.append(APP_PATH + '/bin')\n","\n","# setupなどのOSに関するコマンド実行はその定義ファイルをコマンドで実行\n","# 関数などのモジュールはimportで扱えるようにして実行\n","# !python bin/setup.py"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1683,"status":"ok","timestamp":1684398650740,"user":{"displayName":"佐藤太洋","userId":"13036286077109574300"},"user_tz":-540},"id":"1nLMch089N2C","outputId":"f8657143-d89c-43b0-fade-38da0e1f28b2"},"outputs":[{"output_type":"stream","name":"stdout","text":["image-type： <class 'numpy.ndarray'>\n","image dtype: <class 'PIL.Image.Image'>\n","image mode: RGB\n","image shape: (960, 960, 3)\n","image transpose shape: (3, 960, 960)\n","0 / 290\n","image-type： <class 'numpy.ndarray'>\n","image dtype: <class 'PIL.Image.Image'>\n","image mode: RGB\n","image shape: (960, 960, 3)\n","image transpose shape: (3, 960, 960)\n","1 / 290\n","image-type： <class 'numpy.ndarray'>\n","image dtype: <class 'PIL.Image.Image'>\n","image mode: RGB\n","image shape: (960, 960, 3)\n","image transpose shape: (3, 960, 960)\n","2 / 290\n","image-type： <class 'numpy.ndarray'>\n","image dtype: <class 'PIL.Image.Image'>\n","image mode: RGB\n","image shape: (960, 960, 3)\n","image transpose shape: (3, 960, 960)\n","3 / 290\n","image-type： <class 'numpy.ndarray'>\n","image dtype: <class 'PIL.Image.Image'>\n","image mode: RGB\n","image shape: (960, 960, 3)\n","image transpose shape: (3, 960, 960)\n","4 / 290\n","image-type： <class 'numpy.ndarray'>\n","image dtype: <class 'PIL.Image.Image'>\n","image mode: RGB\n","image shape: (960, 960, 3)\n","image transpose shape: (3, 960, 960)\n","5 / 290\n","image-type： <class 'numpy.ndarray'>\n","image dtype: <class 'PIL.Image.Image'>\n","image mode: RGB\n","image shape: (960, 960, 3)\n","image transpose shape: (3, 960, 960)\n","6 / 290\n","image-type： <class 'numpy.ndarray'>\n","image dtype: <class 'PIL.Image.Image'>\n","image mode: RGB\n","image shape: (960, 960, 3)\n","image transpose shape: (3, 960, 960)\n","7 / 290\n","image-type： <class 'numpy.ndarray'>\n","image dtype: <class 'PIL.Image.Image'>\n","image mode: RGB\n","image shape: (960, 960, 3)\n","image transpose shape: (3, 960, 960)\n","8 / 290\n","image-type： <class 'numpy.ndarray'>\n","image dtype: <class 'PIL.Image.Image'>\n","image mode: RGB\n","image shape: (960, 960, 3)\n","image transpose shape: (3, 960, 960)\n","9 / 290\n","transpose_images dtype: <class 'numpy.ndarray'>\n","transpose_images shape: (10, 3, 960, 960)\n","transpose_images: [[[[ 65  54  47 ... 117 119 115]\n","   [ 74  62  53 ... 110 115 107]\n","   [ 74  88 112 ... 115 113 111]\n","   ...\n","   [ 80  79  89 ...  90  98 134]\n","   [ 84  82 100 ... 239 240 235]\n","   [ 85  84 103 ... 238 236 236]]\n","\n","  [[ 65  54  47 ... 117 119 115]\n","   [ 74  62  53 ... 110 115 107]\n","   [ 74  88 112 ... 115 113 111]\n","   ...\n","   [ 80  79  89 ...  90  98 134]\n","   [ 84  82 100 ... 239 240 235]\n","   [ 85  84 103 ... 238 236 236]]\n","\n","  [[ 65  54  47 ... 117 119 115]\n","   [ 74  62  53 ... 110 115 107]\n","   [ 74  88 112 ... 115 113 111]\n","   ...\n","   [ 80  79  89 ...  90  98 134]\n","   [ 84  82 100 ... 239 240 235]\n","   [ 85  84 103 ... 238 236 236]]]\n","\n","\n"," [[[ 81  82  77 ... 128  99  91]\n","   [ 93  83  86 ... 135 118 112]\n","   [ 90  74  75 ... 138 135 145]\n","   ...\n","   [100 104 120 ... 113 116 122]\n","   [114 115 124 ... 112 121 146]\n","   [119 123 127 ... 116 117 123]]\n","\n","  [[ 81  82  77 ... 128  99  91]\n","   [ 93  83  86 ... 135 118 112]\n","   [ 90  74  75 ... 138 135 145]\n","   ...\n","   [100 104 120 ... 113 116 122]\n","   [114 115 124 ... 112 121 146]\n","   [119 123 127 ... 116 117 123]]\n","\n","  [[ 81  82  77 ... 128  99  91]\n","   [ 93  83  86 ... 135 118 112]\n","   [ 90  74  75 ... 138 135 145]\n","   ...\n","   [100 104 120 ... 113 116 122]\n","   [114 115 124 ... 112 121 146]\n","   [119 123 127 ... 116 117 123]]]\n","\n","\n"," [[[155 158 163 ... 166 166 166]\n","   [156 159 162 ... 166 166 166]\n","   [157 159 162 ... 165 165 165]\n","   ...\n","   [168 170 173 ...  64  82  78]\n","   [168 170 173 ...  68  91  88]\n","   [169 171 173 ...  75 102 100]]\n","\n","  [[155 158 163 ... 166 166 166]\n","   [156 159 162 ... 166 166 166]\n","   [157 159 162 ... 165 165 165]\n","   ...\n","   [168 170 173 ...  64  82  78]\n","   [168 170 173 ...  68  91  88]\n","   [169 171 173 ...  75 102 100]]\n","\n","  [[155 158 163 ... 166 166 166]\n","   [156 159 162 ... 166 166 166]\n","   [157 159 162 ... 165 165 165]\n","   ...\n","   [168 170 173 ...  64  82  78]\n","   [168 170 173 ...  68  91  88]\n","   [169 171 173 ...  75 102 100]]]\n","\n","\n"," ...\n","\n","\n"," [[[204 234 236 ... 243 242 243]\n","   [181 228 234 ... 243 243 243]\n","   [116 218 233 ... 243 242 242]\n","   ...\n","   [236 231 232 ... 226 173 144]\n","   [241 241 241 ... 116 101 118]\n","   [243 242 243 ... 119 110 123]]\n","\n","  [[204 234 236 ... 243 242 243]\n","   [181 228 234 ... 243 243 243]\n","   [116 218 233 ... 243 242 242]\n","   ...\n","   [236 231 232 ... 226 173 144]\n","   [241 241 241 ... 116 101 118]\n","   [243 242 243 ... 119 110 123]]\n","\n","  [[204 234 236 ... 243 242 243]\n","   [181 228 234 ... 243 243 243]\n","   [116 218 233 ... 243 242 242]\n","   ...\n","   [236 231 232 ... 226 173 144]\n","   [241 241 241 ... 116 101 118]\n","   [243 242 243 ... 119 110 123]]]\n","\n","\n"," [[[ 60  50  48 ... 158 158 164]\n","   [ 55  43  38 ... 167 168 153]\n","   [ 69  68  71 ... 160 166 140]\n","   ...\n","   [145 151 158 ... 170 182 187]\n","   [170 156 136 ... 175 179 179]\n","   [153 160 153 ... 173 176 176]]\n","\n","  [[ 60  50  48 ... 158 158 164]\n","   [ 55  43  38 ... 167 168 153]\n","   [ 69  68  71 ... 160 166 140]\n","   ...\n","   [145 151 158 ... 170 182 187]\n","   [170 156 136 ... 175 179 179]\n","   [153 160 153 ... 173 176 176]]\n","\n","  [[ 60  50  48 ... 158 158 164]\n","   [ 55  43  38 ... 167 168 153]\n","   [ 69  68  71 ... 160 166 140]\n","   ...\n","   [145 151 158 ... 170 182 187]\n","   [170 156 136 ... 175 179 179]\n","   [153 160 153 ... 173 176 176]]]\n","\n","\n"," [[[ 81  82  90 ...  87  95 105]\n","   [ 82  82  86 ...  93  92  91]\n","   [ 85  91  98 ... 110 110 100]\n","   ...\n","   [206 200 214 ... 223 219 229]\n","   [216 203 212 ... 184 168 197]\n","   [219 207 213 ... 203 187 187]]\n","\n","  [[ 81  82  90 ...  87  95 105]\n","   [ 82  82  86 ...  93  92  91]\n","   [ 85  91  98 ... 110 110 100]\n","   ...\n","   [206 200 214 ... 223 219 229]\n","   [216 203 212 ... 184 168 197]\n","   [219 207 213 ... 203 187 187]]\n","\n","  [[ 81  82  90 ...  87  95 105]\n","   [ 82  82  86 ...  93  92  91]\n","   [ 85  91  98 ... 110 110 100]\n","   ...\n","   [206 200 214 ... 223 219 229]\n","   [216 203 212 ... 184 168 197]\n","   [219 207 213 ... 203 187 187]]]]\n","transpose_labels dtype: <class 'numpy.ndarray'>\n","transpose_labels shape: (10,)\n","transpose_labels: [5 5 2 3 1 0 2 5 2 4]\n"]}],"source":["# 画像データ(npzのバイナリファイル)の読み込み\n","\n","# 対物レンズ10倍。\n","# 入力サイズ960px\n","# x.1の画像のみのデータセット\n","\n","\n","datasets_npz_x1 = np.load(APP_PATH + '/dataset_npz/x.1_dataset/sc_x10_960_x1.npz')\n","# datasets_npz_x3 = np.load(APP_PATH + '/dataset_npz/x.1_dataset/sc_x10_960_x1.npz')\n","# datasets_npz_all = np.load(APP_PATH + '/dataset_npz/x.1_dataset/sc_x10_960_x1.npz')\n","\n","\n","CATEGORIES=[\"S10C\",\"S15C\",\"S25C\",\"S35C\",\"S45C\",\"S55C\"]\n","\n","\n","# Xはdata, Yはtargetのこと\n","# print(datasets_npz_x1)\n","# print(datasets_npz_x1.files)\n","\n","# データを3次元のtensor([c h w])に変換(1次元と3次元で比較)\n","images = datasets_npz_x1['x']\n","labels = datasets_npz_x1['y']\n","\n","# transpose_images = np.empty(shape=1, dtype=float)\n","# print(\"transpose_images: {}\".format(transpose_images))\n","images_list = []\n","labels_list = []\n","\n","NUM_DATA = 10\n","\n","for i, image in enumerate(images):\n","    # CPUが使い果たされてしまうので200までを使用\n","    if i == NUM_DATA:\n","        break\n","\n","    print('image-type：', type(image))\n","    # 標準化もここで行なう？？\n","    image = Image.fromarray(image)\n","    print(\"image dtype: {}\".format(type(image)))\n","    image = image.convert(\"RGB\")\n","    print(\"image mode: {}\".format(image.mode)) # むりやり3チャンネルにしているので表記上はRGBだが実際はL\n","    image = np.asarray(image, np.uint8)\n","    print(\"image shape: {}\".format(image.shape))\n","    image = np.transpose(image, (2, 0, 1))\n","    print(\"image transpose shape: {}\".format(image.shape))\n","    # transpose_images = np.append(transpose_images, image, axis=0)\n","    images_list.append(image)\n","    labels_list.append(labels[i])\n","    \n","    print(i, '/', len(images))\n","\n","\n","transpose_images = np.array(images_list)\n","transpose_labels = np.array(labels_list)\n","\n","# うまくいっていたら保存\n","print(\"transpose_images dtype: {}\".format(type(transpose_images)))\n","print(\"transpose_images shape: {}\".format(transpose_images.shape))\n","print(\"transpose_images: {}\".format(transpose_images))\n","\n","print(\"transpose_labels dtype: {}\".format(type(transpose_labels)))\n","print(\"transpose_labels shape: {}\".format(transpose_labels.shape))\n","print(\"transpose_labels: {}\".format(transpose_labels))\n","np.savez(APP_PATH, 'data/temp/np_savez', transpose_images, datasets_npz_x1['y'])\n","\n","# image = Image.fromarray(image)\n","# print(\"image dtype: {}\".format(type(image)))\n","# image = image.convert(\"RGB\")\n","# print(\"image mode: {}\".format(image.mode)) # むりやり3チャンネルにしているので表記上はRGBだが実際はL\n","# image = np.asarray(image, np.uint8)\n","# print(\"image shape: {}\".format(image.shape))\n","# image = np.transpose(image, (2, 0, 1))\n","# print(\"image transpose shape: {}\".format(image.shape))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":418,"status":"ok","timestamp":1684398487153,"user":{"displayName":"佐藤太洋","userId":"13036286077109574300"},"user_tz":-540},"id":"kJiD_NPdvl9K","outputId":"bf244159-66e4-4c88-b83d-df11c72c6831"},"outputs":[{"output_type":"stream","name":"stdout","text":["data.shape: torch.Size([10, 3, 960, 960]) label.shape: torch.Size([10])\n","torch.Size([3, 960, 960]) torch.Size([])\n","tensor([[[ 65.,  54.,  47.,  ..., 117., 119., 115.],\n","         [ 74.,  62.,  53.,  ..., 110., 115., 107.],\n","         [ 74.,  88., 112.,  ..., 115., 113., 111.],\n","         ...,\n","         [ 80.,  79.,  89.,  ...,  90.,  98., 134.],\n","         [ 84.,  82., 100.,  ..., 239., 240., 235.],\n","         [ 85.,  84., 103.,  ..., 238., 236., 236.]],\n","\n","        [[ 65.,  54.,  47.,  ..., 117., 119., 115.],\n","         [ 74.,  62.,  53.,  ..., 110., 115., 107.],\n","         [ 74.,  88., 112.,  ..., 115., 113., 111.],\n","         ...,\n","         [ 80.,  79.,  89.,  ...,  90.,  98., 134.],\n","         [ 84.,  82., 100.,  ..., 239., 240., 235.],\n","         [ 85.,  84., 103.,  ..., 238., 236., 236.]],\n","\n","        [[ 65.,  54.,  47.,  ..., 117., 119., 115.],\n","         [ 74.,  62.,  53.,  ..., 110., 115., 107.],\n","         [ 74.,  88., 112.,  ..., 115., 113., 111.],\n","         ...,\n","         [ 80.,  79.,  89.,  ...,  90.,  98., 134.],\n","         [ 84.,  82., 100.,  ..., 239., 240., 235.],\n","         [ 85.,  84., 103.,  ..., 238., 236., 236.]]])\n","tensor(5)\n"]}],"source":["# transformsなしの自作のデータセット作成\n","\n","data = torch.tensor(transpose_images, dtype=torch.float32)\n","label = torch.tensor(transpose_labels, dtype=torch.int64)\n","\n","print('data.shape:', data.shape, 'label.shape:', label.shape)\n","\n","# Datasetを作成\n","dataset = torch.utils.data.TensorDataset(data, label)\n","\n","X_sample, y_sample = dataset[0]\n","print(X_sample.shape, y_sample.shape)\n","print(X_sample)\n","print(y_sample)\n","\n","\n","\n","# transformsを使ったrailにのったやり方(自作クラスを後に作成する)\n","# https://dreamer-uma.com/pytorch-dataset/"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1684398487153,"user":{"displayName":"佐藤太洋","userId":"13036286077109574300"},"user_tz":-540},"id":"Lv-c6LuDvl9L","outputId":"f1c9e9f2-0764-4210-f7f6-2d7ccdbfebed"},"outputs":[{"output_type":"stream","name":"stdout","text":["num_train: 8\n","num_test: 2\n","train: <torch.utils.data.dataset.Subset object at 0x7fbf565dd9c0>\n","test: <torch.utils.data.dataset.Subset object at 0x7fbf565dc460>\n"]}],"source":["# DataLoader ミニバッチ学習\n","# 入力値と目標値をまとめる\n","\n","\n","# 学習データ、検証データ、テストデータに分ける\n","# 各データセットのサンプル数を決定\n","# train : val : test = 60% : 20% : 20%\n","num_train = int(len(dataset) * 0.8)\n","# num_val = int(len(dataset) * 0.2)\n","num_test = int(len(dataset) - num_train)\n","\n","print('num_train:', num_train)\n","# print('num_val:', num_val)\n","print('num_test:', num_test)\n","\n","# ランダムにするらめにシードの固定をした\n","torch.manual_seed(0)\n","# データセットの分割\n","train, test = torch.utils.data.random_split(dataset, [num_train, num_test])\n","print('train:', train)\n","# print('val:', val)\n","print('test:', test)\n","\n","\n","\n","\n","# 一括でデータを作成\n","# train_X, test_X, train_y, test_y = train_test_split(X, y, test_size\n","# =0.8, random_state=1)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1684398487154,"user":{"displayName":"佐藤太洋","userId":"13036286077109574300"},"user_tz":-540},"id":"a7B85skevl9L","outputId":"ed613958-3d8c-413b-a9b9-8553e10688e0"},"outputs":[{"output_type":"stream","name":"stdout","text":["x: tensor([[[[169., 169., 169.,  ..., 178., 169., 181.],\n","          [171., 171., 171.,  ..., 177., 171., 180.],\n","          [174., 174., 174.,  ..., 174., 175., 177.],\n","          ...,\n","          [ 95.,  87.,  80.,  ..., 175., 174., 174.],\n","          [109.,  89.,  88.,  ..., 173., 172., 172.],\n","          [151., 104.,  85.,  ..., 171., 170., 170.]],\n","\n","         [[169., 169., 169.,  ..., 178., 169., 181.],\n","          [171., 171., 171.,  ..., 177., 171., 180.],\n","          [174., 174., 174.,  ..., 174., 175., 177.],\n","          ...,\n","          [ 95.,  87.,  80.,  ..., 175., 174., 174.],\n","          [109.,  89.,  88.,  ..., 173., 172., 172.],\n","          [151., 104.,  85.,  ..., 171., 170., 170.]],\n","\n","         [[169., 169., 169.,  ..., 178., 169., 181.],\n","          [171., 171., 171.,  ..., 177., 171., 180.],\n","          [174., 174., 174.,  ..., 174., 175., 177.],\n","          ...,\n","          [ 95.,  87.,  80.,  ..., 175., 174., 174.],\n","          [109.,  89.,  88.,  ..., 173., 172., 172.],\n","          [151., 104.,  85.,  ..., 171., 170., 170.]]],\n","\n","\n","        [[[ 65.,  54.,  47.,  ..., 117., 119., 115.],\n","          [ 74.,  62.,  53.,  ..., 110., 115., 107.],\n","          [ 74.,  88., 112.,  ..., 115., 113., 111.],\n","          ...,\n","          [ 80.,  79.,  89.,  ...,  90.,  98., 134.],\n","          [ 84.,  82., 100.,  ..., 239., 240., 235.],\n","          [ 85.,  84., 103.,  ..., 238., 236., 236.]],\n","\n","         [[ 65.,  54.,  47.,  ..., 117., 119., 115.],\n","          [ 74.,  62.,  53.,  ..., 110., 115., 107.],\n","          [ 74.,  88., 112.,  ..., 115., 113., 111.],\n","          ...,\n","          [ 80.,  79.,  89.,  ...,  90.,  98., 134.],\n","          [ 84.,  82., 100.,  ..., 239., 240., 235.],\n","          [ 85.,  84., 103.,  ..., 238., 236., 236.]],\n","\n","         [[ 65.,  54.,  47.,  ..., 117., 119., 115.],\n","          [ 74.,  62.,  53.,  ..., 110., 115., 107.],\n","          [ 74.,  88., 112.,  ..., 115., 113., 111.],\n","          ...,\n","          [ 80.,  79.,  89.,  ...,  90.,  98., 134.],\n","          [ 84.,  82., 100.,  ..., 239., 240., 235.],\n","          [ 85.,  84., 103.,  ..., 238., 236., 236.]]]])\n","t: tensor([3, 5])\n"]}],"source":["# ミニバッチ学習\n","\n","# バッチサイズの定義(目安は全データの1/10だが正解がない)\n","batch_size = 2\n","train_loader = torch.utils.data.DataLoader(train, batch_size, shuffle=True, drop_last=True)\n","# val_loader = torch.utils.data.DataLoader(val, batch_size)\n","test_loader = torch.utils.data.DataLoader(test, batch_size)\n","\n","# 1つ目のバッチサイズに分割したデータを確認\n","x, t = next(iter(train_loader))\n","\n","print('x:', x)\n","print('t:', t)\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5776,"status":"ok","timestamp":1684398492927,"user":{"displayName":"佐藤太洋","userId":"13036286077109574300"},"user_tz":-540},"id":"lw8crVe2vl9L","outputId":"b0afc83e-e5ce-47ce-8970-2db8902277bb"},"outputs":[{"output_type":"stream","name":"stdout","text":["AlexNet(\n","  (features): Sequential(\n","    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(1, 1), padding=(2, 2))\n","    (1): ReLU(inplace=True)\n","    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n","    (4): ReLU(inplace=True)\n","    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (7): ReLU(inplace=True)\n","    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (9): ReLU(inplace=True)\n","    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (11): ReLU(inplace=True)\n","    (12): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  )\n","  (classifier): Sequential(\n","    (0): Dropout(p=0.5, inplace=False)\n","    (1): Linear(in_features=2304, out_features=4096, bias=True)\n","    (2): ReLU(inplace=True)\n","    (3): Dropout(p=0.5, inplace=False)\n","    (4): Linear(in_features=4096, out_features=4096, bias=True)\n","    (5): ReLU(inplace=True)\n","    (6): Linear(in_features=4096, out_features=6, bias=True)\n","  )\n",")\n","Device: cuda\n"]}],"source":["# 2.3 ニューラルネットワークの定義\n","\n","num_classes = 6    # CIFAR10のクラスの数を指定\n","\n","class AlexNet(nn.Module):\n","    def __init__(self, num_classes=num_classes):\n","        super(AlexNet, self).__init__()\n","        self.features = nn.Sequential(\n","            nn.Conv2d(3, 64, kernel_size=7, padding=2),     # 畳み込み層\n","            nn.ReLU(inplace=True),                          # 活性化関数\n","            nn.MaxPool2d(kernel_size=2),                    # プーリング層\n","            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(kernel_size=2),\n","            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(kernel_size=2),\n","        )\n","        self.classifier = nn.Sequential(\n","            nn.Dropout(),                   # ドロップアウト層\n","            nn.Linear(256 * 3* 3, 4096),\n","            nn.ReLU(inplace=True),\n","            nn.Dropout(),\n","            nn.Linear(4096, 4096),\n","            nn.ReLU(inplace=True),\n","            nn.Linear(4096, num_classes),\n","        )\n","\n","    # 順伝搬を定義\n","    def forward(self, x):\n","        x = self.features(x)                # 画像特徴量抽出パート\n","        x = x.view(x.size(0), 256 * 3 * 3)  # 3次元から1次元に変えて全結合層へ\n","        x = self.classifier(x)              # 画像分類パート\n","        return x\n","        \n","# ネットワークのロード\n","# CPUとGPUのどちらを使うかを指定\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","net = AlexNet().to(device)\n","print(net)\n","# デバイスの確認\n","print(\"Device: {}\".format(device))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R868b0zHvl9L"},"outputs":[],"source":["# 2.4 損失関数と最適化関数の定義\n","\n","# 損失関数の定義\n","criterion = nn.CrossEntropyLoss()\n","\n","# 活性化関数の定義\n","optimizer = optim.Adam(net.parameters())"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":420},"executionInfo":{"elapsed":7769,"status":"error","timestamp":1684398500684,"user":{"displayName":"佐藤太洋","userId":"13036286077109574300"},"user_tz":-540},"id":"Mk4X6bEQemHX","outputId":"12938d8a-381a-43cf-b013-78c5fdd70d7b"},"outputs":[{"output_type":"stream","name":"stdout","text":["-------------------------------\n","Epoch: 1/10\n"]},{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-9-3760d0af56ac>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;31m# データを入力して予測値を計算(順伝播)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0my_pred_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0;31m# 損失(誤差)を計算\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred_prob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-7-652113099fc7>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m                \u001b[0;31m# 画像特徴量抽出パート\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m256\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 3次元から1次元に変えて全結合層へ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m              \u001b[0;31m# 画像分類パート\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: shape '[2, 2304]' is invalid for input of size 7250432"]}],"source":["# 2.5 学習\n","\n","# 損失と正解率を保持するリストを作成\n","train_loss_list = []        # 学習損失\n","train_accuracy_list = []    # 学習データの正答率\n","test_loss_list = []         # 評価損失\n","test_accuracy_list = []     # テストデータの正答率\n","\n","# 学習(エポック)の実行\n","epoch = 10\n","for i in range(epoch):\n","    # エポックの進行状況を表示\n","    print(\"-------------------------------\")\n","    print(\"Epoch: {}/{}\".format(i+1, epoch))\n","    \n","    # 損失と正解率の初期化\n","    train_loss = 0      # 学習損失\n","    train_accuracy = 0  # 学習データの正答率\n","    test_loss = 0       # 評価損失\n","    test_accuracy = 0   # テストデータの正答率\n","    \n","    # ---------学習パート--------- #\n","    # ニューラルネットワークを学習モードに設定\n","    net.train()\n","    # ミニバッチごとにデータをロードし学習\n","    for images, labels in train_loader:\n","        # GPUにTensorを転送\n","        images = images.to(device)\n","        labels = labels.to(device)\n","        \n","        # 勾配を初期化\n","        optimizer.zero_grad()\n","        # データを入力して予測値を計算(順伝播)\n","        y_pred_prob = net(images)\n","        # 損失(誤差)を計算\n","        loss = criterion(y_pred_prob, labels)\n","        # 勾配の計算(逆伝搬)\n","        loss.backward()\n","        # パラメータ(重み)の更新\n","        optimizer.step()\n","        \n","        # ミニバッチごとの損失を蓄積\n","        train_loss += loss.item()\n","        \n","        # 予測したラベルを予測確率y_pred_probから計算\n","        y_pred_labels = torch.max(y_pred_prob, 1)[1]\n","        # ミニバッチごとに正解したラベル数をカウント\n","        train_accuracy += torch.sum(y_pred_labels == labels).item() / len(labels)\n","    \n","    # エポックごとの損失と正解率を計算(ミニバッチの平均の損失と正解率を計算)\n","    epoch_train_loss = train_loss / len(train_loader)\n","    epoch_train_accuracy = train_accuracy / len(train_loader)\n","    # ---------学習パートはここまで--------- #\n","    \n","    # ---------評価パート--------- #\n","    # ニューラルネットワークを評価モードに設定\n","    net.eval()\n","    # 評価時の計算で自動微分機能をオフにする\n","    with torch.no_grad():\n","        for images, labels in test_loader:\n","            # GPUにTensorを転送\n","            images = images.to(device)\n","            labels = labels.to(device)\n","            # データを入力して予測値を計算(順伝播)\n","            y_pred_prob = net(images)\n","            # 損失(誤差)を計算\n","            loss = criterion(y_pred_prob, labels)\n","            # ミニバッチごとの損失を蓄積\n","            test_loss += loss.item()\n","            \n","            # 予測したラベルを予測確率y_pred_probから計算\n","            y_pred_labels = torch.max(y_pred_prob, 1)[1]\n","            # ミニバッチごとに正解したラベル数をカウント\n","            test_accuracy += torch.sum(y_pred_labels == labels).item() / len(labels)\n","        \n","    # エポックごとの損失と正解率を計算(ミニバッチの平均の損失と正解率を計算)\n","    epoch_test_loss = test_loss / len(test_loader)\n","    epoch_test_accuracy = test_accuracy / len(test_loader)\n","    # ---------評価パートはここまで--------- #\n","\n","    # エポックごとに損失と正解率を表示\n","    print(\"Train_Loss: {:.4f}, Train_Accuracy: {:.4f}\".format(epoch_train_loss, epoch_train_accuracy))\n","    print(\"Test_Loss: {:.4f}, Test_Accuracy: {:.4f}\".format(epoch_test_loss, epoch_test_accuracy))\n","\n","    # 損失と正解率をリスト化して保存\n","    train_loss_list.append(epoch_train_loss)\n","    train_accuracy_list.append(epoch_train_accuracy)\n","    test_loss_list.append(epoch_test_loss)\n","    test_accuracy_list.append(epoch_test_accuracy)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WiN--Ax7emHX"},"outputs":[],"source":["# 2.6 結果の可視化\n","\n","# 損失\n","plt.figure()\n","plt.title('Train and Test Loss')\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","plt.plot(range(1, epoch+1), train_loss_list, color='blue', linestyle='-', label='Train_Loss')\n","plt.plot(range(1, epoch+1), test_loss_list, color='red', linestyle='--', label='Test_Loss')\n","plt.legend()    # 凡例\n","\n","# 正解率\n","plt.figure()\n","plt.title('Train and Test Accuracy')\n","plt.xlabel('Epoch')\n","plt.ylabel('Accuracy')\n","plt.plot(range(1, epoch+1), train_accuracy_list, color='blue', linestyle='-', label='Train_Accuracy')\n","plt.plot(range(1, epoch+1), test_accuracy_list, color='red', linestyle='--', label='Test_Accuracy')\n","plt.legend()    # 凡例\n","\n","# 表示\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"va1ja350emHY"},"outputs":[],"source":["# 分類した画像を確認\n","# CIFAR10のクラス\n","# classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']  # CIFAR10のクラス\n","CATEGORIES=[\"S10C\",\"S15C\",\"S25C\",\"S35C\",\"S45C\",\"S55C\"]\n","\n","\n","# ニューラルネットワークを評価モードに設定\n","net.eval()\n","# 評価時の計算で自動微分機能をオフにする\n","with torch.no_grad():\n","    for images, labels in test_loader:\n","        # GPUにTensorを転送\n","        images = images.to(device)\n","        labels = labels.to(device)\n","        # データを入力して予測値を計算(順伝播)\n","        y_pred_prob = net(images)\n","        # 予測したラベルを予測確率y_pred_probから計算\n","        y_pred_labels = torch.max(y_pred_prob, 1)[1]\n","        \n","        for i in range(9):\n","            image = images[i] / 2 + 0.5\n","            image = image.to('cpu').numpy()\n","            plt.subplot(3, 3, i+1)\n","            # matplotlibでは(縦, 横, チャネル)の順\n","            plt.imshow(np.transpose(image, (1, 2, 0)))\n","            plt.title(CATEGORIES[labels[i]])\n","            plt.title(\"{} ({})\".format(CATEGORIES[y_pred_labels[i].item()], CATEGORIES[labels[i].item()]), color=('green' if y_pred_labels[i] == labels[i] else 'red'))\n","            plt.axis('off')\n","        plt.show()\n","        break\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"84ezOFPzemHY"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":0}